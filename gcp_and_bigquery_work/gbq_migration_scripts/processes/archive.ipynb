{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e82e4727-bbd9-4950-9c3f-d53fa53cc051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fcb4c13e-b504-4032-8e3d-255e1feb6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/jupyter/gitlab/2023-pols-na-dna-spring-cleaning/files/archive.csv\"\n",
    "logfile = \"/home/jupyter/gitlab/2023-pols-na-dna-spring-cleaning/logs/archive.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eced55d0-4831-473d-9e5a-f4470f26604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fbff1bb8-cf3b-449d-beb8-b31077b0e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=\"polsbigquery\")\n",
    "bigquery_client = bigquery.Client(project=\"polsbigquery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e0bfd620-c1d1-4273-bc87-ad6293d7005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bucket_exists(bucket):\n",
    "    \"\"\"\n",
    "    NAME: check_bucket_exists\n",
    "    \n",
    "    DESCRIPTION: Checks if the bucket exists\n",
    "    \n",
    "    PARAMETERS:\n",
    "        bucket(str): name of the bucket\n",
    "        \n",
    "    RETURNS\n",
    "        True/False\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def upload_to_bucket(bucket, project, dataset, table):\n",
    "    \n",
    "    \"\"\"\n",
    "    NAME: upload_to_bucket\n",
    "    \n",
    "    DESCRIPTION: Uploads the file to the bucket and sets the storage class\n",
    "    \n",
    "    PARAMETERS:\n",
    "        bucket(str): name of the bucket\n",
    "        project(str): name of the project\n",
    "        dataset(str): name of the dataset\n",
    "        table(str): name of the table\n",
    "        \n",
    "    RETURNS:\n",
    "        msg(str): message to be sent to logfile with action time\n",
    "    \"\"\"\n",
    "    \n",
    "    #STORAGE_CLASS = 'NEARLINE'\n",
    "    #STORAGE_CLASS = 'COLDLINE'\n",
    "    #STORAGE_CLASS = 'ARCHIVE'\n",
    "    STORAGE_CLASS = 'STANDARD'\n",
    "    \n",
    "    #Setting variables\n",
    "    filename = f\"\"\"{bucket}/{dataset}/{re.sub('[.]', '_', table)}.csv\"\"\"\n",
    "    filepath = f\"gs://{filename}\"\n",
    "    table_name = f\"{project}.{dataset}.{table}\"\n",
    "    blobname= dataset+\"/\"+re.sub('[.]', '_', table)+'.csv'\n",
    "    \n",
    "    print(f\"Uploading {table_name} to {filepath} with storage class: {STORAGE_CLASS}.\")\n",
    "    \n",
    "    # Getting table reference\n",
    "    table_ref = bigquery_client.get_table(table_name)\n",
    "    \n",
    "    # Creating and excuting the Extract Table job\n",
    "    job_config = bigquery.ExtractJobConfig()\n",
    "    extract_job = bigquery_client.extract_table(\n",
    "        table_ref,\n",
    "        filepath,\n",
    "        job_config=job_config\n",
    "    )\n",
    "    extract_job.result()\n",
    "    \n",
    "    #Getting the blob reference after extracting\n",
    "    bucket = storage_client.get_bucket(bucket)\n",
    "    blob = bucket.blob(blobname)\n",
    "    \n",
    "    #Setting the storage class\n",
    "    blob.update_storage_class(STORAGE_CLASS)\n",
    "    \n",
    "    #Creating message\n",
    "    msg = f\"\"\"\n",
    "New Location: {filepath} \n",
    "Storage Class: {STORAGE_CLASS}\n",
    "Transfer Time: {datetime.datetime.now().isoformat()}\\n\"\"\"\n",
    "    \n",
    "    return msg    \n",
    "    \n",
    "def capture_log_data(table_name):   \n",
    "    \n",
    "    \"\"\"\n",
    "    NAME: capture_log_data\n",
    "    \n",
    "    DESCRIPTION: Captures log data from the table instance before deletion\n",
    "    \n",
    "    PARAMETERS:\n",
    "        table_name(str): name of the table instance\n",
    "        \n",
    "    RETURNS\n",
    "        msg(str): message to be sent to logfile with table metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capture table metadata\n",
    "    table_ref = bigquery_client.get_table(table_name)\n",
    "    \n",
    "    # setting variables\n",
    "    table_id = table_ref.table_id\n",
    "    path = table_ref.path\n",
    "    num_rows = table_ref.num_rows\n",
    "    num_bytes = table_ref.num_bytes\n",
    "    modified = table_ref.modified\n",
    "    created = table_ref.created\n",
    "    schema = table_ref.schema\n",
    "    \n",
    "    # making messsage\n",
    "    msg = f\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "table_id: {table_id}\n",
    "path: {path}\n",
    "num_rows: {num_rows}\n",
    "num_bytes: {num_bytes}\n",
    "modified: {modified}\n",
    "created: {created}\n",
    "schema: {schema}\n",
    "\"\"\"\n",
    "    \n",
    "    return msg\n",
    "    \n",
    "def delete_table(table_name):\n",
    "    \"\"\"\n",
    "    NAME: delete_table\n",
    "    \n",
    "    DESCRIPTION: Deletes the table\n",
    "    \n",
    "    PARAMETERS:\n",
    "        table_name(str): name of the table\n",
    "        \n",
    "    RETURNS:\n",
    "        msg(str): message to be sent to logfile with action time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting table reference\n",
    "    table_ref = bigquery_client.get_table(table_name)\n",
    "    \n",
    "    # Deleting the table\n",
    "    bigquery_client.delete_table(table_ref)\n",
    "    \n",
    "    #Creating message\n",
    "    msg = f\"\"\"Table Action: Deleted {table_name} at {datetime.datetime.now().isoformat()}.\"\"\"\n",
    "    \n",
    "    return msg\n",
    "\n",
    "def write_to_log_file(file, msg):\n",
    "    \"\"\"\n",
    "    NAME: write_to_log_file\n",
    "    \n",
    "    DESCRIPTION: Writes message to the log file\n",
    "    \n",
    "    PARAMETERS:\n",
    "        file(str): name of the log file\n",
    "        msg(str): message to be sent to logfile\n",
    "    \"\"\"\n",
    "    \n",
    "    # append to log file\n",
    "    \n",
    "    with open(file, 'a') as f:\n",
    "        f.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "906bcb12-a99a-42c8-ad66-cb146365dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket does not exist, creating...edwa_global_archive\n",
      "Uploading polsbigquery.skunkworks.test_table_1 to gs://edwa_global_archive/skunkworks/test_table_1.csv with storage class: STANDARD.\n",
      "Uploading polsbigquery.skunkworks.test_table_2 to gs://edwa_global_archive/skunkworks/test_table_2.csv with storage class: STANDARD.\n",
      "Uploading polsbigquery.skunkworks.test_table_3 to gs://edwa_global_archive/skunkworks/test_table_3.csv with storage class: STANDARD.\n"
     ]
    }
   ],
   "source": [
    "for r in range(0,len(df)):\n",
    "    \n",
    "    project = df['project'][r]\n",
    "    dataset = df['schema'][r]\n",
    "    table = df['table'][r]\n",
    "    action = df['action'][r]\n",
    "    \n",
    "    if action == 'archive':\n",
    "    \n",
    "        bucket = \"edwa_global_archive\"\n",
    "        table_name = project+\".\"+dataset+\".\"+table\n",
    "\n",
    "        # check if cloud storage bucket exists.  If not, create it.\n",
    "        if check_bucket_exists(bucket):\n",
    "            #print(bucket)\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Bucket does not exist, creating...{bucket}\")\n",
    "            storage_client.create_bucket(bucket)\n",
    "\n",
    "        # Copy table to cloud storage bucket, place in long term storage\n",
    "        upload_msg = upload_to_bucket(bucket, project, dataset, table)\n",
    "        \n",
    "        # Capture log data\n",
    "        metadata_msg = capture_log_data(table_name)\n",
    "        \n",
    "        # Delete table\n",
    "        delete_msg = delete_table(table_name)\n",
    "        \n",
    "        # Cobble together messages\n",
    "        msg = metadata_msg + upload_msg + delete_msg\n",
    "        \n",
    "        # Send message to log file\n",
    "        write_to_log_file(logfile, msg)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
