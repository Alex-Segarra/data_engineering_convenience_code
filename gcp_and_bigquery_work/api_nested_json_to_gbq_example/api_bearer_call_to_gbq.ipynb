{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud.bigquery import SchemaField\n",
    "from google.cloud import secretmanager\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import io\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '170117011701' #polsnadna-nonprd'\n",
    "secret_id = 'secret_survey_export_credentials'\n",
    "version_id = 'latest'\n",
    "bucket = \"survey_exports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_secret(secret_id, PROJECT_ID, version_id=\"latest\"):\n",
    "        \n",
    "    # Create the Secret Manager client.\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version.\n",
    "    name = f\"projects/{PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version.\n",
    "    response = client.access_secret_version(name=name)\n",
    "\n",
    "    # Return the decoded payload.\n",
    "    return response.payload.data.decode('UTF-8')\n",
    "\n",
    "def authenticate(url, json):\n",
    "   \n",
    "    print(f'1.a Authenticating')\n",
    "    response = requests.post(url = url, json = json)\n",
    "    print(f'1.b Status Code: {response.status_code}')\n",
    "    return(response)\n",
    "\n",
    "def post_reporting_request(url, params, token_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sends request to API to generate report.\n",
    "    \n",
    "    :param url: URL for request\n",
    "    :param params: Dictionary, list of tuples or bytes to send\n",
    "        in the query string.  Contains three key:value pairs    \n",
    "        EXAMPLE:\n",
    "            params={\n",
    "              \"start_date\": \"2022-06-01T00:00:00.000+00:00\",\n",
    "                \"end_date\": \"2022-06-02T00:00:00.000+00:00\",\n",
    "                  \"format\": \"JSON\"\n",
    "                    } \n",
    "    :param token_dict: Dictionary, lists authentication and time valid.\n",
    "    \n",
    "    :return: :class:`Response <Response>` object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'2.a Posting request for {url}')\n",
    "    \n",
    "    token = str(token_dict['auth_token'])\n",
    "\n",
    "    headers = {\n",
    "       'accept': 'application/json',\n",
    "        'Authorization': f\"Bearer {token}\"\n",
    "        }\n",
    "\n",
    "    response = requests.post(url=url, headers=headers, json=params)\n",
    "    \n",
    "    print(f'2.b Status Code: {response.status_code}')\n",
    "    \n",
    "    return(response)\n",
    "\n",
    "def parse_request_id(response):\n",
    "    \n",
    "    if response.status_code == 400:\n",
    "        \n",
    "        desc = response.json()['error_description']\n",
    "        \n",
    "        print(desc)\n",
    "\n",
    "        request_id = re.sub('^(.* )([A-Za-z0-9\\-]+)','\\\\2',desc)\n",
    "    \n",
    "    elif response.status_code == 200:\n",
    "\n",
    "        request_id = eval(response.text)['request_id']\n",
    "        \n",
    "    return(request_id)\n",
    "\n",
    "def submit_report_request(endpoint,params,token_dict):\n",
    "    \n",
    "    report_req = post_reporting_request(url=endpoint, params=params, token_dict=token_dict)\n",
    "    \n",
    "    request_id = parse_request_id(response = report_req)\n",
    "    \n",
    "    return(request_id)\n",
    "\n",
    "\n",
    "def get_report_job_uri(endpoint, request_id, token_dict):\n",
    "\n",
    "    token = str(token_dict['auth_token'])\n",
    "\n",
    "    headers_ = {\n",
    "       'accept': 'application/json',\n",
    "        'Authorization': f\"Bearer {token}\"\n",
    "        }\n",
    "\n",
    "    url = endpoint + request_id\n",
    "    \n",
    "    response = requests.get(url,headers=headers_)\n",
    "    \n",
    "    while eval(response.text)['status'] == \"INPROGRESS\":\n",
    "        print('Report generation in progress.  Trying again in 5 seconds.')\n",
    "        time.sleep(5)\n",
    "    \n",
    "        response = requests.get(url,headers=headers_)\n",
    "        \n",
    "    return(response)\n",
    "\n",
    "def get_report_from_uri(response):\n",
    "    \n",
    "    report_url = eval(response.text)['url']\n",
    "    \n",
    "    report = requests.get(url=report_url)\n",
    "    \n",
    "    return(report)\n",
    "\n",
    "def fully_flatten_json(df):\n",
    "    \n",
    "    #Catch unparseable columns here\n",
    "    bad_cols = []\n",
    "    \n",
    "    #Conditional Loop - while there are any lists or dicts in columns, keep loop going.  Will ignore columns flagged as \"bad\"\n",
    "    while\\\n",
    "        df[df.columns[~df.columns.isin(bad_cols)]].apply(lambda y: y.apply(lambda x: isinstance(x,list))).any().any()\\\n",
    "    or\\\n",
    "        df[df.columns[~df.columns.isin(bad_cols)]].apply(lambda y: y.apply(lambda x: isinstance(x,dict))).any().any():\n",
    "    \n",
    "    \n",
    "        #Parses lists first.  This creates a list of columns with lists to \"explode.\"\n",
    "        cols = [x for x in df.columns if df[x].apply(lambda z: isinstance(z,list)).any()]\n",
    "\n",
    "        for c in cols:\n",
    "            \n",
    "            print(f'Parsing lists in column {c}')\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                #Joins exploded list to old dataframe\n",
    "                df = pd.concat([df[c].explode(), df.drop(columns=[c])], axis=1).reset_index(drop=True)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                #If any issues present themselves, it just skips and adds to the list of bad columns\n",
    "                print(f'Issue parsing {c}.  Skipping.')\n",
    "                bad_cols.append(c)\n",
    "                \n",
    "                continue\n",
    "\n",
    "            #Finding any dicts within the recently parsed lists\n",
    "            if df.apply(lambda y: y.apply(lambda x: isinstance(x,dict))).any().any():\n",
    "\n",
    "                #Cycles through ID'd dict columns\n",
    "                cols = [x for x in df.columns if df[x].apply(lambda z: isinstance(z,dict)).any()]\n",
    "                \n",
    "                for c in cols:\n",
    "                    \n",
    "                    print(f'Parsing dicts in column {c}')\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        print(f'JSON Normalizing {c}')\n",
    "\n",
    "                        tdf=pd.json_normalize(df[c])\n",
    "                        \n",
    "                    except AttributeError as e:\n",
    "                        \n",
    "                        #This triggers if there are rows without inputs mixed with rows with dicts.  It will simply fill in nan rows with empty dicts, save for column names\n",
    "                        \n",
    "                        print(f'Got {e}.  Trying another way.')\n",
    "                        \n",
    "                        #This gets the column names\n",
    "                        colnames = set([i for j in [k.keys() for k in df[c].dropna()] for i in j])\n",
    "                        \n",
    "                        #This parses with the new \"insert\" columns\n",
    "                        tdf = pd.json_normalize(df[c].where(df[c].notna(), lambda d: {x:np.nan for x in colnames}))\n",
    "        \n",
    "                        pass\n",
    "\n",
    "                    except:\n",
    "                        \n",
    "                        #This triggers for unsolvable errors parsing\n",
    "                        print(f'Issue parsing {c}.  Skipping')\n",
    "                        \n",
    "                        bad_cols.append(c)\n",
    "                        \n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        \n",
    "                        #This adds parsed JSON columns to Dataframe\n",
    "                        print(f'Joining {c} to DF')\n",
    "\n",
    "                        df = pd.concat([tdf.rename(columns={x:f\"{c}_{x}\" for x in tdf.columns})\\\n",
    "                                    ,df.drop(columns=[c])], axis=1)\\\n",
    "                        .reset_index(drop=True)\n",
    "                        \n",
    "                    except:\n",
    "                        \n",
    "                        #This triggers for unsolvable errors parsing\n",
    "\n",
    "                        print(f'Issue parsing {c}.  Skipping.')\n",
    "\n",
    "                        bad_cols.append(c)\n",
    "\n",
    "                        continue\n",
    "    \n",
    "    #Get rid of pesky '.' in column names            \n",
    "    df.columns = [re.sub('[.]{1}','_',x) for x in df.columns]\n",
    "    \n",
    "    print('Writing to dataframe.')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.a Authenticating\n",
      "1.b Status Code: 200\n",
      "Getting Report for evaluations/\n",
      "Pinging Endpoint: https://kong.observe.ai/v1/data/reports/evaluations/\n",
      "Getting Request ID\n",
      "2.a Posting request for https://kong.observe.ai/v1/data/reports/evaluations/\n",
      "2.b Status Code: 400\n",
      "similar job already exists with request id 9e9d06d0-b308-43ce-a5fa-ed384adfddc4\n",
      "Getting Report URI\n",
      "Getting Report from URI\n",
      "Parsing.\n",
      "Parsing lists in column value.evaluation_forms\n",
      "Parsing dicts in column value.evaluation_forms\n",
      "JSON Normalizing value.evaluation_forms\n",
      "Joining value.evaluation_forms to DF\n",
      "Parsing lists in column value.evaluation_forms_evaluations\n",
      "Parsing dicts in column value.evaluation_forms_evaluations\n",
      "JSON Normalizing value.evaluation_forms_evaluations\n",
      "Joining value.evaluation_forms_evaluations to DF\n",
      "Parsing lists in column value.evaluation_forms_template.sections\n",
      "Parsing dicts in column value.evaluation_forms_template.sections\n",
      "JSON Normalizing value.evaluation_forms_template.sections\n",
      "Joining value.evaluation_forms_template.sections to DF\n",
      "Parsing lists in column value.evaluation_forms_template.sections_questions\n",
      "Parsing dicts in column value.evaluation_forms_template.sections_questions\n",
      "JSON Normalizing value.evaluation_forms_template.sections_questions\n",
      "Joining value.evaluation_forms_template.sections_questions to DF\n",
      "Parsing lists in column value.evaluation_forms_evaluations_evaluation_list\n",
      "Parsing dicts in column value.evaluation_forms_evaluations_evaluation_list\n",
      "JSON Normalizing value.evaluation_forms_evaluations_evaluation_list\n",
      "Joining value.evaluation_forms_evaluations_evaluation_list to DF\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.teamName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.lastName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.postQueueSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.agentId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.abandonSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.routingTime\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isLogged\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.holdCount\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isAnalyticsProcessed\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isShortAbandon\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.primaryDispositionId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.inQueueSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.skillName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.callbackTime\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.releaseSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.agentSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.preQueueSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.abandoned\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.serviceLevelFlag\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.toAddr\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.mediaTypeName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isRefused\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isOutbound\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.contactId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.pointOfContactName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.campaignId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.masterContactId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.mediaType\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.pointOfContactId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.secondaryDispositionId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.transferIndicatorId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.ACWSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.skillId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.firstName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.isTakeover\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.confSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.transferIndicatorName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.analyticsProcessedDate\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.teamId\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.holdSeconds\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.fromAddr\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.campaignName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.refuseReason\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.dispositionNotes\n",
      "Parsing lists in column value.evaluation_forms_evaluations_channel_meta.dispositionName\n",
      "Parsing lists in column value.evaluation_forms_evaluations_evaluation_list_response.sections\n",
      "Parsing dicts in column value.evaluation_forms_evaluations_evaluation_list_response.sections\n",
      "JSON Normalizing value.evaluation_forms_evaluations_evaluation_list_response.sections\n",
      "Joining value.evaluation_forms_evaluations_evaluation_list_response.sections to DF\n",
      "Parsing lists in column value.evaluation_forms_evaluations_evaluation_list_response.sections_questions\n",
      "Parsing dicts in column value.evaluation_forms_evaluations_evaluation_list_response.sections_questions\n",
      "JSON Normalizing value.evaluation_forms_evaluations_evaluation_list_response.sections_questions\n",
      "Joining value.evaluation_forms_evaluations_evaluation_list_response.sections_questions to DF\n",
      "Writing to dataframe.\n",
      "Sending to GBQ\n"
     ]
    }
   ],
   "source": [
    "auth_headers = eval(get_google_secret(secret_id, PROJECT_ID))\n",
    "auth_endpoint = 'https://ghidorah.survey.ai/v1/oauth/token'\n",
    "\n",
    "url_base = \"https://ghidorah.survey.ai/v1/data/reports/\"\n",
    "\n",
    "#report_list = ['interactions/','evaluations/','coachings/']\n",
    "#report_list = ['interactions/']\n",
    "report_list = ['evaluations/']\n",
    "\n",
    "params={\n",
    "  \"start_date\": \"2022-06-01T00:00:00.000+00:00\",\n",
    "  \"end_date\": \"2022-06-02T00:00:00.000+00:00\",\n",
    "  \"format\": \"JSON\"\n",
    "}\n",
    "\n",
    "auth_token = authenticate(url=auth_endpoint, json=auth_headers)\n",
    "\n",
    "token_dict = eval(auth_token.content.decode('utf-8'))\n",
    "\n",
    "for r in report_list:\n",
    "    \n",
    "    print(f'Getting Report for {r}')\n",
    "    \n",
    "    endpoint = url_base+r\n",
    "    \n",
    "    print(f'Pinging Endpoint: {endpoint}')\n",
    "    \n",
    "    print(f'Getting Request ID')\n",
    "\n",
    "    request_id = submit_report_request(endpoint=endpoint, params=params, token_dict=token_dict)\n",
    "    \n",
    "    print(f'Getting Report URI')\n",
    "\n",
    "    response=get_report_job_uri(endpoint=endpoint, request_id=request_id,  token_dict=token_dict)\n",
    "    \n",
    "    print(f'Getting Report from URI')\n",
    "\n",
    "    report = get_report_from_uri(response=response)\n",
    "    \n",
    "    print('Parsing.')\n",
    "\n",
    "    df = fully_flatten_json(pd.json_normalize(report.json()))\n",
    "    \n",
    "    print('Sending to GBQ')\n",
    "\n",
    "    df.to_gbq(destination_table=f'skunkworks.survey_data_{re.sub(r\"/+\",\"\",r)}_report_test', project_id='project-1', if_exists='replace')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
